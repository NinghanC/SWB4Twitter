{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042add1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[14]:\n",
    "\n",
    "\n",
    "# Importing the libraries needed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import transformers\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaModel, XLMRobertaTokenizer\n",
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "import matplotlib\n",
    "from tqdm import tqdm, trange\n",
    "matplotlib.use('AGG')\n",
    "import matplotlib.pyplot as plt\n",
    "import transformers\n",
    "import torch\n",
    "import demoji\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "# Setting up the device for GPU usage\n",
    "\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "DN = 'Senti'\n",
    "# Defining dataset and key variables that will be used later on in the training\n",
    "\n",
    "\n",
    "\n",
    "if DN == 'Senti':\n",
    "    MAX_LEN = 128\n",
    "    TRAIN_BATCH_SIZE = 64\n",
    "    VALID_BATCH_SIZE = 32\n",
    "    EPOCHS = 15\n",
    "    LEARNING_RATE = 1e-05\n",
    "    tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-large')\n",
    "    FRAC = 0.001\n",
    "    NUM_LABELS = 2\n",
    "    \n",
    "    \n",
    "    data = pd.read_pickle('../Data/SemEval2017_data')\n",
    "    print(data.head())\n",
    "    data_1 = data[['text','label']]\n",
    "    data_1 = data_1.rename({'text': 'Phrase',\n",
    "                           'label':'Sentiment'}, axis='columns')\n",
    "    new_df = data_1.sample(frac= FRAC, replace=False, random_state=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SentimentData(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.text = dataframe.Phrase\n",
    "        self.targets = self.data.Sentiment\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.text[index])\n",
    "        text = \" \".join(text.split())\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n",
    "        }\n",
    "\n",
    "\n",
    "# In[17]:\n",
    "\n",
    "\n",
    "train_size = 0.8\n",
    "train_data=new_df.sample(frac=train_size,random_state=200)\n",
    "test_data=new_df.drop(train_data.index).reset_index(drop=True)\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(new_df.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_data.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_data.shape))\n",
    "\n",
    "training_set = SentimentData(train_data, tokenizer, MAX_LEN)\n",
    "testing_set = SentimentData(test_data, tokenizer, MAX_LEN)\n",
    "\n",
    "\n",
    "# In[18]:\n",
    "\n",
    "\n",
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)\n",
    "# In[20]:\n",
    "\n",
    "\n",
    "\n",
    "class RobertaClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RobertaClass, self).__init__()\n",
    "        self.l1 = RobertaModel.from_pretrained(\"xlm-roberta-large\")\n",
    "        self.pre_classifier = torch.nn.Linear(1024, 1024)\n",
    "        self.dropout = torch.nn.Dropout(0.5)\n",
    "        self.classifier = torch.nn.Linear(1024, NUM_LABELS)\n",
    "        self.act = torch.nn.Softmax(1)\n",
    "        self.id =  datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "\n",
    "    def forward(self, input_ids, attention_mask,token_type_ids):\n",
    "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        hidden_state = output_1[0]\n",
    "        pooler = hidden_state[:, 0]\n",
    "        #print('hidden',hidden_state.shape)\n",
    "        pooler = self.pre_classifier(pooler)\n",
    "        #print('pre_classifier',pooler.shape)\n",
    "        pooler = torch.nn.ReLU()(pooler)\n",
    "        #print('ReLU',pooler.shape)\n",
    "        pooler = self.dropout(pooler)\n",
    "        #print('dropout',pooler.shape)\n",
    "        pooler = self.classifier(pooler)\n",
    "        #print('classifier',pooler.shape)\n",
    "        output = self.act(pooler)\n",
    "        #print('act',output.shape)\n",
    "        return output\n",
    "\n",
    "\n",
    "model = RobertaClass()\n",
    "model.to(device)\n",
    "\n",
    "dr = '../Model/{}_{}_{}/'.format(model.id,DN,FRAC)\n",
    "if not os.path.exists(dr):\n",
    "    os.makedirs(dr)\n",
    "\n",
    "# In[23]:\n",
    "\n",
    "\n",
    "# The loss function and optimizer\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "\n",
    "# In[24]:\n",
    "\n",
    "\n",
    "def calcuate_accuracy(preds, targets):\n",
    "    n_correct = (preds==targets).sum().item()\n",
    "    return n_correct\n",
    "\n",
    "train_loss_set = []\n",
    "val_loss_set = []\n",
    "val_flat_accuracy_set = []\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# Defining the training function on the 80% of the dataset for tuning\n",
    "\n",
    "def train(epoch):\n",
    "    tr_loss = 0\n",
    "    n_correct = 0\n",
    "    nb_tr_steps = 0\n",
    "    nb_tr_examples = 0\n",
    "    model.train()\n",
    "    for _,data in tqdm(enumerate(training_loader, 0)):\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.long)\n",
    "        outputs = model(ids, mask,token_type_ids)\n",
    "        #print('targets',targets)\n",
    "        #print('outputs',outputs)\n",
    "        loss = loss_function(outputs, targets)\n",
    "        tr_loss += loss.item()\n",
    "        big_val, big_idx = torch.max(outputs.data, dim=1)\n",
    "        n_correct += calcuate_accuracy(big_idx, targets)\n",
    "\n",
    "        nb_tr_steps += 1\n",
    "        nb_tr_examples+=targets.size(0)\n",
    "        \n",
    "        if _%5000==0:\n",
    "            loss_step = tr_loss/nb_tr_steps\n",
    "            accu_step = (n_correct*100)/nb_tr_examples \n",
    "            print(f\"Training Loss per 5000 steps: {loss_step}\")\n",
    "            print(f\"Training Accuracy per 5000 steps: {accu_step}\")\n",
    "            with open(dr + 'file.txt', 'a') as f:\n",
    "                print(f\"Training Loss per 5000 steps: {loss_step}\",file=f)\n",
    "                print(f\"Training Accuracy per 5000 steps: {accu_step}\",file=f)\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # # When using GPU\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'The Total Accuracy for Epoch {epoch}: {(n_correct*100)/nb_tr_examples}')\n",
    "    with open(dr + 'file.txt', 'a') as f:\n",
    "        print(f'The Total Accuracy for Epoch {epoch}: {(n_correct)/nb_tr_examples}',file=f)\n",
    "    epoch_loss = tr_loss/nb_tr_steps\n",
    "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
    "    print(f\"Training Loss Epoch: {epoch_loss}\")\n",
    "    print(f\"Training Accuracy Epoch: {epoch_accu}\")\n",
    "    with open(dr + 'file.txt', 'a') as f:\n",
    "        print(f\"Training Loss Epoch: {epoch_loss}\",file=f)\n",
    "    with open(dr + 'file.txt', 'a') as f:\n",
    "        print(f\"Training Accuracy Epoch: {epoch_accu}\",file=f)\n",
    "\n",
    "    return \n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "for epoch in trange(EPOCHS):\n",
    "    train(epoch)\n",
    "\n",
    "\n",
    "# In[25]:\n",
    "\n",
    "\n",
    "def valid(model, testing_loader):\n",
    "    model.eval()\n",
    "    n_correct = 0; n_wrong = 0; total = 0; tr_loss=0; nb_tr_steps=0; nb_tr_examples=0\n",
    "    with torch.no_grad():\n",
    "        for _, data in tqdm(enumerate(testing_loader, 0)):\n",
    "            ids = data['ids'].to(device, dtype = torch.long)\n",
    "            mask = data['mask'].to(device, dtype = torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n",
    "            targets = data['targets'].to(device, dtype = torch.long)\n",
    "            outputs = model(ids, mask, token_type_ids).squeeze()\n",
    "            loss = loss_function(outputs, targets)\n",
    "            tr_loss += loss.item()\n",
    "            big_val, big_idx = torch.max(outputs.data, dim=1)\n",
    "            n_correct += calcuate_accuracy(big_idx, targets)\n",
    "\n",
    "            nb_tr_steps += 1\n",
    "            nb_tr_examples+=targets.size(0)\n",
    "            \n",
    "            if _%5000==0:\n",
    "                loss_step = tr_loss/nb_tr_steps\n",
    "                accu_step = (n_correct*100)/nb_tr_examples\n",
    "                print(f\"Validation Loss per 100 steps: {loss_step}\")\n",
    "                print(f\"Validation Accuracy per 100 steps: {accu_step}\")\n",
    "    epoch_loss = tr_loss/nb_tr_steps\n",
    "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
    "    print(f\"Validation Loss Epoch: {epoch_loss}\")\n",
    "    print(f\"Validation Accuracy Epoch: {epoch_accu}\")\n",
    "    with open(dr + 'file.txt', 'a') as f:\n",
    "        print(f\"Validation Loss Epoch: {epoch_loss}\",file=f)\n",
    "    with open(dr + 'file.txt', 'a') as f:\n",
    "        print(f\"Validation Accuracy Epoch: {epoch_accu}\",file=f)\n",
    "    \n",
    "    \n",
    "    return epoch_accu\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "acc = valid(model, testing_loader)\n",
    "print(\"Accuracy on test data = %0.2f%%\" % acc)\n",
    "with open(dr + 'file.txt', 'a') as f:\n",
    "    print(\"Accuracy on test data = %0.2f%%\" % acc,file=f)\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "output_model_file = dr + 'pytorch_roberta_sentiment.bin'\n",
    "#output_vocab_file = dr + 'output_vocab_file/'\n",
    "\n",
    "model_to_save = model\n",
    "torch.save(model_to_save, output_model_file)\n",
    "#tokenizer.save_vocabulary(output_vocab_file)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
